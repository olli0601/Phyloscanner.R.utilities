# load the two utility functions 
take_bridging_individual <- function(df_graph,df){
  # gather bridging individuals 
  # inputs: 
  # df_graph: similarity between individuals,
  # df: individuals and cluster memberships.
  # outputs: 
  # all the bridging individuals.
  bridging = list()
  for (k in 1:max(df$MEMBERSHIP)) {
    tmp1 = df[MEMBERSHIP==k,]$PT_ID
    tmp2 = df_graph[pt_id1 %in% tmp1 & !(pt_id2 %in% tmp1) & SIMILARITY >=0.975,]
    tmp3 = df_graph[pt_id2 %in% tmp1 & !(pt_id1 %in% tmp1) & SIMILARITY >=0.975,]
    bridging[[k]] = unique(c(as.character(tmp2$pt_id1),  
                             as.character(tmp3$pt_id2)))
  }
  return(bridging)
}
break_large_clusters <- function(idlarge, dflarge, ddist){
  # break large clusters into subclusters and clusters of bridging individuals
  # inputs: 
  # idlarge: id of large clusters,
  # dflarge: individuals that belong to large clusters,
  # ddist: similarity between individuals.
  # outputs:
  # subclusters generated by breaking the large clusters.
  ans <- data.table()
  for (i in idlarge) {
    idlarge_ind <- dflarge[MEMBERSHIP==i]$PT_ID
    maxsize <- unique(dflarge[MEMBERSHIP==i,]$CLU_SIZE)
    ddist_idlarge_ind <- ddist[pt_id1 %in% idlarge_ind & pt_id2 %in% idlarge_ind]
    chain_idlarge_ind <- graph.data.frame(ddist_idlarge_ind, directed=FALSE, vertices=NULL)
    E(chain_idlarge_ind)$weight <- ddist_idlarge_ind$SIMILARITY
    chain_idlarge_ind <- simplify(chain_idlarge_ind)
    E(chain_idlarge_ind)$weight = ifelse(E(chain_idlarge_ind)$weight > 1,E(chain_idlarge_ind)$weight/2, E(chain_idlarge_ind)$weight)
    tmp = clusters(chain_idlarge_ind, mode='weak')
    stopifnot(all(tmp$membership==1))
    comm_idlarge_ind <-cluster_louvain(chain_idlarge_ind)
    df_idlarge_ind = data.table(PT_ID=comm_idlarge_ind$names,MEMBERSHIP = comm_idlarge_ind$membership)
    df_graph_idlarge_ind = as_long_data_frame(chain_idlarge_ind)
    df_graph_idlarge_ind = data.table(df_graph_idlarge_ind[,3:5])
    colnames(df_graph_idlarge_ind) = c('SIMILARITY','pt_id1','pt_id2')
    bridging <- take_bridging_individual(df_graph_idlarge_ind,df_idlarge_ind)  
    bridging_all <- unique(do.call(c,bridging))
    if(length(bridging_all)!=0){
      tmpidclu <- max(comm_idlarge_ind$membership)+1
      df_idlarge_ind = rbind(df_idlarge_ind, data.table(PT_ID=bridging_all,MEMBERSHIP = tmpidclu))  
    }
    tmp = df_idlarge_ind[,list(CLU_SIZE = length(PT_ID)),by='MEMBERSHIP']
    df_idlarge_ind = merge(df_idlarge_ind, tmp,by='MEMBERSHIP')
    df_idlarge_ind[,IDC:=i]
    ans <- rbind(ans,df_idlarge_ind)
  }
  tmp <- unique(subset(ans,select=c('MEMBERSHIP','IDC')))
  tmp[,MEMBERSHIP2:=seq_len(nrow(tmp))]
  ans <- merge(ans, tmp, by=c('MEMBERSHIP','IDC'))
  ans[,MEMBERSHIP:=NULL]
  ans[,IDC:=NULL]
  setnames(ans,'MEMBERSHIP2','MEMBERSHIP')
  return(ans)
}

library(data.table)
library(ggplot2)
library(seqinr)
library(ggpubr)
library(tidyverse)
library(igraph)
library(dplyr)

close.pairs <- function(){
  # identify close pairs using the threshold 0.5.
  # main inputs: 
  # infile.support: the proportion of windows for close pairs between sequences,
  # cutoff: the threshold for close pairs.
  # main outputs: 
  # close_pairs: close pairs of individuals.

  # file names
  data.dir <- '/rds/general/project/ratmann_pangea_deepsequencedata/live/'
  out.base <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/210120_RCCSUVRI_'
  potential.networks.analysis.dir <- "/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/200929_SIMILARTY_windowsize500_batchsize100"
  infile.support <- file.path(potential.networks.analysis.dir ,paste0('support_overall_method2_sequence_level_processed_v4.rda'))
  infile.ind.rccs <- file.path(data.dir,'PANGEA2_RCCS/200316_pangea_db_sharing_extract_rakai.csv')
  infile.ind.mrc <- file.path(data.dir,'PANGEA2_MRC/200319_pangea_db_sharing_extract_mrc.csv')
  cutoff <- 0.5
  
  # load
  load(infile.support)
  df <- df[DATA >=4,]
  df <- df[PROP >= cutoff]
  
  # map to individuals
  id.dt <- data.table(read.csv(infile.ind.rccs))
  id.dt <- subset(id.dt,select = c("pt_id","sex","pangea_id"))
  id.dt[,pangea_id:=paste0('RCCS_',pangea_id)]
  tmp <- data.table(read.csv(infile.ind.mrc))
  tmp <- subset(tmp,select = c("pt_id","sex","pangea_id"))
  tmp[,pangea_id:=paste0('MRCUVRI_',pangea_id)]
  id.dt <- rbind(id.dt,tmp)
  id.dt <- unique(id.dt)
  setnames(id.dt, colnames(id.dt), paste0(colnames(id.dt),'1'))
  df <- merge(df,id.dt, by.x=c('TAXA1'), by.y=c('pangea_id1'), all.x=T)
  setnames(id.dt, colnames(id.dt), gsub('1','2',colnames(id.dt)))
  df <- merge(df,id.dt, by.x=c('TAXA2'), by.y=c('pangea_id2'), all.x=T)
  setnames(id.dt, colnames(id.dt), gsub('2','',colnames(id.dt)))
  df = df[!is.na(pt_id1) & !is.na(pt_id2)]
  
  # remove pairs of sequences that belong to the same person
  close_pairs <- unique(subset(df,select=c('pt_id1', 'pt_id2')))
  close_pairs <- close_pairs[pt_id1!=pt_id2]
  close_pairs[,pt_id1:= as.character(pt_id1)]
  close_pairs[,pt_id2:= as.character(pt_id2)]
  
  # save
  save(close_pairs,file=file.path(potential.networks.analysis.dir,'close_pairs_cutoffprop05.rda'))
}



identify.clusters <- function(){
  # identify clusters using the close pairs.
  # main inputs:
  #  infile.close.paris: close pairs,
  #  infile.ddist: similarity between consensus sequences.
  # main outputs: 
  #  clusters.
  
  # directories
  data.dir <- '/rds/general/project/ratmann_pangea_deepsequencedata/live/'
  out.base <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/210120_RCCSUVRI_'
  potential.networks.analysis.dir <- "/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/200929_SIMILARTY_windowsize500_batchsize100"
  
  # files
  infile.couple <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/RakaiPangeaMetaData_v2.rda'
  infile.ind.rccs <- file.path(data.dir,'PANGEA2_RCCS/200316_pangea_db_sharing_extract_rakai.csv')
  infile.ind.mrc <- file.path(data.dir,'PANGEA2_MRC/200319_pangea_db_sharing_extract_mrc.csv')
  infile.close.paris <- file.path(potential.networks.analysis.dir,'close_pairs_cutoffprop05.rda')
  infile.ddist <- file.path(potential.networks.analysis.dir,'ddist.rda')
  
  # make clusters
  load(infile.close.paris)
  chains<- graph.data.frame(close_pairs, directed=FALSE, vertices=NULL)
  rtc	<- data.table(ID=V(chains)$name, CLU=clusters(chains, mode="weak")$membership)
  
  # cluster sizes
  tmp	<- rtc[, list(CLU_SIZE=length(ID)), by='CLU']
  setkey(tmp, CLU_SIZE)
  print(tail(tmp))
  tmp[, IDCLU:=rev(seq_len(nrow(tmp)))]
  rtc	<- subset( merge(rtc, tmp, by='CLU'))
  rtc[, CLU:=NULL]
  setkey(rtc, IDCLU)
  
  # take individuals from the largest cluster - 1291
  id = rtc[IDCLU==1,]$ID
  chains<- subset(close_pairs, select=c(pt_id1, pt_id2))
  chains<- chains[pt_id1 %in% id & pt_id2 %in% id ]

  # process the largest cluster and break it into several subclusters
  load(infile.ddist)
  ddist = copy(ddist_copy)  
  ddist[SIMILARITY>=0.975, SIMILARITY:=1]
  ddist <- ddist[,list(SIMILARITY = mean(SIMILARITY)),by=c('pt_id1','pt_id2')]
  chains_tmp<- graph.data.frame(ddist, directed=FALSE, vertices=NULL)
  E(chains_tmp)$weight = ddist$SIMILARITY
  chains_tmp <- simplify(chains_tmp)
  E(chains_tmp)$weight = ifelse(E(chains_tmp)$weight > 1,E(chains_tmp)$weight/2, E(chains_tmp)$weight)
  comm <-cluster_louvain(chains_tmp)
  df = data.table(PT_ID=comm$names,MEMBERSHIP = comm$membership)
  df_graph = as_long_data_frame(chains_tmp)
  df_graph = data.table(df_graph[,3:5])
  colnames(df_graph) = c('SIMILARITY','pt_id1','pt_id2')
  bridging <- take_bridging_individual(df_graph,df)  
  bridging_all <- unique(do.call(c,bridging))
  cat('number of clusters: ',length(unique(df$MEMBERSHIP)),'\n')
  id_clu <- max(df$MEMBERSHIP) 
  tmp <- data.table(PT_ID=bridging_all, MEMBERSHIP=id_clu+1)
  df <- rbind(df, tmp)
  df = rbind(df, data.table(PT_ID=bridging_all,MEMBERSHIP = id_clu))
  tmp = df[,list(CLU_SIZE = length(PT_ID)),by='MEMBERSHIP']
  df = merge(df, tmp,by='MEMBERSHIP')
  
  # apply the trick to all the clusters of sizes larger than 55
  dflarge <- df[CLU_SIZE>55]
  df <- df[CLU_SIZE<=55]
  tmp <- data.table(MEMBERSHIP2= seq_len(length(unique(df$MEMBERSHIP))),
                    MEMBERSHIP=unique(df$MEMBERSHIP))
  df <- merge(df,tmp, by='MEMBERSHIP')
  df[,MEMBERSHIP:=NULL]
  setnames(df,'MEMBERSHIP2','MEMBERSHIP')
  idlarge <- unique(dflarge$MEMBERSHIP)
  cat('\n The IDs of clusters of size > 55 are \n')
  print(idlarge)
  breaked_dflarge <- break_large_clusters(idlarge, dflarge, ddist)
  
  # repeat the trick utill all the cluster sizes < 55
  while (T) {
    id_clu <- max(df$MEMBERSHIP)
    tmp <- breaked_dflarge[CLU_SIZE <= 55]
    tmp2 <- unique(subset(tmp,select=c('CLU_SIZE','MEMBERSHIP')))
    setkey(tmp2,MEMBERSHIP)
    tmp2[,MEMBERSHIP2:=seq_len(nrow(tmp2))]
    tmp <- merge(tmp, tmp2, by=c('CLU_SIZE','MEMBERSHIP'))
    tmp[,MEMBERSHIP:=NULL]
    setnames(tmp,'MEMBERSHIP2','MEMBERSHIP')
    tmp[, MEMBERSHIP:=MEMBERSHIP+id_clu]
    df <- rbind(df, tmp)
    dflarge <- breaked_dflarge[CLU_SIZE>55]
    idlarge <- unique(dflarge$MEMBERSHIP)
    print(idlarge)
    if(length(idlarge)==0) break
    breaked_dflarge <- break_large_clusters(idlarge, dflarge,ddist)
  }
  
  # combined with the small clusters
  setnames(df,c('PT_ID','MEMBERSHIP'),c('ID','IDCLU'))
  rtc = rtc[IDCLU!=1,]
  rtc[,IDCLU:=IDCLU + max(df$IDCLU)-1]
  df = rbind(rtc,df)
  
  # save 
  cat('Write clusters to ',file.path(potential.networks.analysis.dir,'clusters_cutoff05.rda'),'...\n')
  save(df,file = file.path(potential.networks.analysis.dir,'clusters_cutoff05.rda'))
}

rkuvri.make.phyloscanner.input.runs <- function()
{
  # add controls to each cluster
  # main inputs:
  #   infile.potential.networks: clusters of individuals,
  #   infile.clostest.three.per.cluster (optional): three closest individuals to all the individuals in a cluster,
  #   infile.couple: known couples.
  #   pty.runs:  the file mapping individuals, samples, and storage locations
  # main outputs:
  #   individuals assigned to each of the phyloscanner runs
  
  # directories
  data.dir <- '/rds/general/project/ratmann_pangea_deepsequencedata/live/'
  out.base <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/210120_RCCSUVRI_'	
  potential.networks.analysis.dir <- "/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/200929_SIMILARTY_windowsize500_batchsize100"
  
  # files
  infile.potential.networks <- file.path(potential.networks.analysis.dir, 'clusters_cutoff05.rda')
  infile.clostest.three.per.cluster <- file.path(potential.networks.analysis.dir, 'closest_3_percluster_cutoff05.rds')
  infile.couple <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/RakaiPangeaMetaData_v2.rda'
  pty.runs <- readRDS(paste0(out.base,'phscinput_samples.rds'))  
  
  # load potential transmission networks of close pairs 
  # make IDCLU ordered by CLU_SIZE
  load( infile.potential.networks )  
  tmp <- unique(subset(df,select=c('CLU_SIZE','IDCLU')))
  setkey(tmp, CLU_SIZE)
  tmp[, IDCLU2:=seq_len(nrow(tmp))]  
  tmp[, CLU_SIZE:=NULL]
  rtc	<- subset( merge(df, tmp, by='IDCLU'),select=c('ID','IDCLU2','CLU_SIZE'))
  setnames(rtc,'IDCLU2','IDCLU')
  setkey(rtc, IDCLU)

  #	find which couples are in potential transmission networks
  load(infile.couple)	
  rp <- data.table(unique(subset(coupdat,select=c('male.RCCS_studyid', 'female.RCCS_studyid'))))
  setnames(rp, c('male.RCCS_studyid', 'female.RCCS_studyid'), c('MALE_RID','FEMALE_RID'))
  rp <- subset(rp, MALE_RID!=FEMALE_RID)
  rp[,FEMALE_RID:=paste0('RK-',FEMALE_RID)]
  rp[,MALE_RID:=paste0('RK-',MALE_RID)]
  tmp <- sort(unique(as.character(pty.runs[['UNIT_ID']])))	
  rp <- unique(subset(rp, FEMALE_RID%in%tmp & MALE_RID%in%tmp, select=c(FEMALE_RID,MALE_RID)))
  setnames(rp, 'FEMALE_RID', 'ID')
  rp <- merge(rp, subset(rtc, select=c(ID, IDCLU)), all.x=1, by='ID')
  setnames(rp, c('ID','IDCLU','MALE_RID'), c('FEMALE_RID', 'FEMALE_IDCLU','ID'))
  rp <- merge(rp, subset(rtc, select=c(ID, IDCLU)), all.x=1, by='ID')
  setnames(rp, c('ID','IDCLU'), c('MALE_RID', 'MALE_IDCLU'))
  rp[, COUP_ID := seq_len(nrow(rp))]
  
  #	reassign partners that are not in the same network as their partner
  tmp <- subset(rp, !is.na(FEMALE_IDCLU) & !is.na(MALE_IDCLU))
  tmp2 <- tmp[, list(NOT_IN_SAME = !any(FEMALE_IDCLU == MALE_IDCLU)),by=c('MALE_RID','FEMALE_RID')]
  tmp2 <- subset(tmp2, NOT_IN_SAME)	
  tmp2 <- merge(tmp2, rp, by=c('MALE_RID','FEMALE_RID'))	
  tmp <- rp[, which(COUP_ID %in% tmp2$COUP_ID)]
  set(rp, tmp, 'MALE_IDCLU', rp[tmp, FEMALE_IDCLU])
  set(tmp2, NULL, 'FEMALE_IDCLU', tmp2[,MALE_IDCLU])
  set(tmp2, NULL, 'NOT_IN_SAME', NULL)
  rp <- rbind(rp, tmp2)
  
  #	assign partners that are in no network to the same potential transmission network as their partner
  tmp <- rp[, which(is.na(FEMALE_IDCLU) & !is.na(MALE_IDCLU))]
  set(rp, tmp, 'FEMALE_IDCLU', rp[tmp, MALE_IDCLU])
  tmp <- rp[, which(!is.na(FEMALE_IDCLU) & is.na(MALE_IDCLU))]
  set(rp, tmp, 'MALE_IDCLU', rp[tmp, FEMALE_IDCLU])
  
  #	make a new potential transmission network for couples that are in no network so far
  tmp <- rp[, which(is.na(FEMALE_IDCLU) & is.na(MALE_IDCLU))]
  set(rp, tmp, c('FEMALE_IDCLU','MALE_IDCLU'), rtc[, max(IDCLU)]+seq_along(tmp))
  
  #	calculate cluster size
  setnames(rp, c('MALE_IDCLU'), c('IDCLU'))
  rp <- subset(melt(rp, id.vars=c('IDCLU'), measure.vars=c('MALE_RID','FEMALE_RID'), value.name='ID'), select=c(ID, IDCLU))
  rtc <- unique(rbind(subset(rtc,select=c('ID','IDCLU')), rp))
  tmp <- rtc[, list(CLU_SIZE=length(ID)), by='IDCLU']
  setkey(tmp, CLU_SIZE)
  tmp[, IDCLU2:=seq_len(nrow(tmp))]	
  rtc <- subset(merge(rtc, tmp, by='IDCLU'),select=c('ID','IDCLU2','CLU_SIZE'))
  setnames(rtc, 'IDCLU2', 'IDCLU')
  setkey(rtc,IDCLU)

  # merge small runs
  tn	<- 8
  tmp	<- unique(subset(rtc, select=c(IDCLU, CLU_SIZE)))
  tmp[, PTY_RUN:= IDCLU]
  tmp[, PTY_SIZE:= CLU_SIZE]
  # https://stackoverflow.com/questions/49076769/dplyr-r-cumulative-sum-with-reset
  sum_reset_at <- function(thresh) {
    function(x) {
      accumulate(x, ~if_else(.x+.y>thresh, .y, .x+.y))
    }  
  }
  tmp <- tmp %>% mutate(PTY_SIZE2 = sum_reset_at(tn)(PTY_SIZE))
  tmp <- as.data.table(tmp)	
  tmp2 <- which(diff(tmp$PTY_SIZE2)<0) + 1
  tmp[, PTY_RUN2 := 0]	
  tmp[c(tmp2,max(tmp2):nrow(tmp)), PTY_RUN2:=1]
  tmp[,PTY_RUN2:=cumsum(PTY_RUN2)]
  stopifnot(sum(tmp[,cumsum(PTY_SIZE),by='PTY_RUN2']$V1 !=tmp$PTY_SIZE2)==0)
  tmp[,PTY_SIZE2:=max(PTY_SIZE2),by='PTY_RUN2']
  tmp[,PTY_RUN2:=PTY_RUN2+1]
  set(tmp,NULL,c('PTY_RUN', 'PTY_SIZE'),NULL)
  setnames(tmp,c('PTY_RUN2', 'PTY_SIZE2'), c('PTY_RUN', 'PTY_SIZE'))
  rtc <- merge(rtc,tmp,by=c('IDCLU', 'CLU_SIZE'))
  
  # find 3 closest individuals to all individuals in each cluster
  if(file.exists(infile.clostest.three.per.cluster))
  {
    dcl <- readRDS(infile.clostest.three.per.cluster)
  }else{
    # load meta
    id.dt <- data.table(read.csv(infile.ind.rccs))
    id.dt <- subset(id.dt,select = c("pt_id","sex","pangea_id"))
    id.dt[,pangea_id:=paste0('RCCS_',pangea_id)]
    tmp <- data.table(read.csv(infile.ind.mrc))
    tmp <- subset(tmp,select = c("pt_id","sex","pangea_id"))
    tmp[,pangea_id:=paste0('MRCUVRI_',pangea_id)]
    id.dt <- rbind(id.dt,tmp)
    id.dt <- unique(id.dt)
    # calculate
    infile.average.distance.per.pair <- file.path(potential.networks.analysis.dir, 'average_distance_perpair.rds')
    if(file.exists(infile.average.distance.per.pair))
    {
      ddist <- readRDS(infile.average.distance.per.pair)
    }else{
      infile.average.distance.per.pair.per.window <- file.path(potential.networks.analysis.dir, 'distance_overall_method2_sequence_level_v4.rda')
      load(infile.average.distance.per.pair.per.window)
      ddist <- subset(distance,select=c('TAXA1','TAXA2'))
      ddist$DIST <- rowMeans(distance[,3:ncol(distance)],na.rm = TRUE)
      setnames(id.dt, colnames(id.dt),paste0(colnames(id.dt),'1'))
      ddist <- merge(ddist, id.dt, by.x='TAXA1', by.y='PANGEA_ID1',all.x=TRUE)
      setnames(id.dt, colnames(id.dt),gsub('1','2',colnames(id.dt)))
      ddist <- merge(ddist, id.dt, by.x='TAXA2', by.y='PANGEA_ID2',all.x=TRUE)
      setnames(id.dt, colnames(id.dt),gsub('2','',colnames(id.dt)))
      ddist <- ddist[,list(DIST=mean(DIST)),by=c('UNIT_ID1','UNIT_ID2')]
      saveRDS(ddist,file=infile.average.distance.per.pair) 
    }
    dcl <- rtc[,{
      tmp <- ddist[UNIT_ID1 %in% ID, c('UNIT_ID2','DIST')]
      tmp2 <- ddist[UNIT_ID2 %in% ID, c('UNIT_ID1','DIST')]
      tmp <- rbind(tmp,tmp2,use.names=F)
      colnames(tmp) <- c('UNIT_ID','DIST')
      tmp <- tmp[!UNIT_ID%in% ID,]
      tmp <- tmp[,list(mean(DIST)),by='UNIT_ID']
      tmp <- tmp[order(V1),]
      list(ID_CLOSE1=tmp$UNIT_ID[nrow(tmp)],DISTANCE1=tmp$V1[nrow(tmp)],
           ID_CLOSE2=tmp$UNIT_ID[nrow(tmp)-1],DISTANCE2=tmp$V1[nrow(tmp)-1],
           ID_CLOSE3=tmp$UNIT_ID[nrow(tmp)-2],DISTANCE3=tmp$V1[nrow(tmp)-2])
    },by=c('CLU_SIZE','IDCLU')]
    saveRDS(dcl,file=infile.clostest.three.per.cluster)
  }
  
  # add the 3 closest individuals to each potential transmission network
  tmp <- subset(dcl, select=c('CLU_SIZE', 'IDCLU', 'ID_CLOSE1', 'ID_CLOSE2', 'ID_CLOSE3'))
  tmp <- melt(tmp, id.vars=c('CLU_SIZE', 'IDCLU'), variable.name= 'CLOSEST', value.name='ID')
  tmp[,CLOSEST:=as.integer(gsub('ID_CLOSE','',CLOSEST))]
  tmp2 <- unique(subset(rtc,select=c('IDCLU','PTY_RUN')))
  set(rtc, NULL ,c('PTY_SIZE', 'PTY_RUN'), NULL)
  set(tmp, NULL, 'CLOSEST',NULL)
  tmp[, ID_TYPE:='control']
  rtc[, ID_TYPE:='target']
  rtc <- rbind(rtc,tmp)
  rtc <- unique(rtc)
  rtc[, CLU_SIZE:=length(ID),by='IDCLU']
  rtc <- merge(rtc, tmp2, by='IDCLU')
  rtc[, PTY_SIZE:=length(ID),by='PTY_RUN']
  
  cat(max(rtc$IDCLU),' clusters of ',length(unique(rtc$ID)), ' individuals, the ten largest cluster sizes are ', 
      unique(subset(rtc,select=c('CLU_SIZE','IDCLU')))$CLU_SIZE[(max(rtc$IDCLU)-10+1):max(rtc$IDCLU)], ', ', nrow(rtc)-length(unique(rtc$ID)),
      'individuals appear in more than one clusters','\n')
  
  # combine with sample info
  setnames(rtc, 'ID', 'UNIT_ID')
  pty.runs <- merge(rtc, pty.runs, by='UNIT_ID',all.x=T)
  
  # write processed samples 
  cat('\nWriting to file ', paste0(out.base,'phscinput_runs_cutoff05.rds') )
  saveRDS(pty.runs, file=paste0(out.base,'phscinput_runs_cutoff05.rds'))
}


rkuvri.make.alignments<-function()
{
  # set up alignment scripts.
  # inputs: 
  # pty.runs: run info including UNIT_ID (individual ID), PTY_RUN (which run), ID_TYPE(control or not),
  # SAMPLE_ID (storage), RENAME_ID (sequenced ID),
  # infile.consensus and infile.consensus.oneeach: consensus sequences.
  # outputs:
  # generate and submit the scripts to make alignment.
  
  require(Phyloscanner.R.utilities)
  library(data.table)
  library(seqinr)
  set.seed(42)
  
  # directories
  prog.pty <- '~/phyloscanner/phyloscanner_make_trees.py'
  HOME <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/'	
  data.dir <- '/rds/general/project/ratmann_pangea_deepsequencedata/live/'
  potential.networks.analysis.dir <- "/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/200929_SIMILARTY_windowsize500_batchsize100"
  in.dir <- file.path(HOME,'211220_phsc_input')		
  work.dir <- file.path(HOME,"211220_phsc_work")
  out.dir <- file.path(HOME,"211220_phsc_output")	
  package.dir <- file.path(.libPaths(),'Phyloscanner.R.utilities')
  dir.create(in.dir)
  dir.create(work.dir)
  dir.create(out.dir)
  
  # files
  downsample <- 200
  infile.runs <- paste0(HOME,'210120_RCCSUVRI_phscinput_runs_cutoff05.rds')
  infile.consensus <- file.path(in.dir,'ConsensusGenomes.fasta')
  infile.consensus.oneeach <- file.path(in.dir,'2019_New_ConsensusGenomesOneEach_GeneCut.fasta')
 
  # load runs
  pty.runs <- readRDS(infile.runs)
  max.per.run <- 4900
  
  # remove same bam files in each run
  setorder(pty.runs,PTY_RUN,-ID_TYPE,UNIT_ID)
  tmp <- pty.runs[,duplicated(SAMPLE_ID),by='PTY_RUN']
  tmp <- tmp[,which(V1)]
  pty.runs <- pty.runs[-tmp,]
  tmp <- pty.runs[,length(SAMPLE_ID)-length(unique(SAMPLE_ID)),by='PTY_RUN']
  stopifnot(all(tmp$V1==0))
  
  # load backgrounds
  consensus_seq<- seqinr::read.fasta(infile.consensus)  
  consensus_seq_names <- names(consensus_seq)
  
  # take hxb2
  hxb2 <- grep('HXB2',names(consensus_seq),value = T)
  hxb2_seq <- consensus_seq[[hxb2]]
  
  # take root
  root.seq <-grep('^REF_CON_M$|REF_CONSENSUS_M$|^REF_CON_H$|REF_CONSENSUS_H$', names(consensus_seq),value = T)
  
  # adapt format
  pty.runs[ID_TYPE=='control',UNIT_ID:=paste0('CNTRL-',UNIT_ID)]
  pty.runs[ID_TYPE=='control',RENAME_ID:=paste0('CNTRL-',RENAME_ID)]
  pty.runs[,BAM:=paste0(data.dir,SAMPLE_ID,'.bam')]
  pty.runs[,REF:=paste0(data.dir,SAMPLE_ID,'_ref.fasta')]
  setkey(pty.runs,PTY_RUN,RENAME_ID)
  
  # remove start, end, vloops
  ptyi <- seq(800,9175,25) 
  ptyi <- c( ptyi[ptyi <= 6615-250],6825,6850,ptyi[ptyi >= 7636]) 
  pty.c	<- lapply(seq_along(ptyi), function(i)
  {
    cat('---------------------------------------------------------------- \n')
    print(i)
    pty.args <- list(prog.pty=prog.pty, 
                     prog.mafft='\" mafft --globalpair --maxiterate 1000 \" ',
                     data.dir=data.dir, 
                     work.dir=work.dir, 
                     out.dir=out.dir, 
                     alignments.file=infile.consensus,
                     alignments.root=root.seq,
                     alignments.pairwise.to=hxb2,
                     window.automatic= '', 
                     merge.threshold=0, 
                     min.read.count=1, 
                     quality.trim.ends=23, 
                     min.internal.quality=23, 
                     merge.paired.reads=TRUE, 
                     discard.improper.pairs=TRUE,
                     no.trees=TRUE, 
                     dont.check.duplicates=FALSE,
                     dont.check.recombination=TRUE,
                     num.bootstraps=1,
                     all.bootstrap.trees=TRUE,
                     strip.max.len=350, 
                     min.ureads.individual=NA, 
                     win=c(ptyi[i],ptyi[i]+250,25,250),				 				
                     keep.overhangs=FALSE,
                     mem.save=0,
                     verbose=TRUE,					
                     select=NA,
                     default.coord=TRUE,
                     realignment=TRUE
    )											
    pty.c <- phsc.cmd.phyloscanner.multi(pty.runs, pty.args)
    pty.c[, W_FROM:= ptyi[i]]
    pty.c
  })
  pty.c	<- do.call('rbind', pty.c)	
  setkey(pty.c,PTY_RUN,W_FROM)
  pty.c[, CASE_ID:= rep(1:max.per.run,times=ceiling(nrow(pty.c)/max.per.run))[1:nrow(pty.c)]]
  pty.c[, JOB_ID:= rep(1:ceiling(nrow(pty.c)/max.per.run),each=max.per.run)[1:nrow(pty.c)]]

  #	define PBS variables
  hpc.load			<- "module load intel-suite/2015.1 mpi raxml/8.2.9 mafft/7 anaconda/2.3.0 samtools"	# make third party requirements available	 
  hpc.select			<- 1						# number of nodes
  hpc.nproc			<- 1						# number of processors on node
  hpc.walltime		<- 71					# walltime
  hpc.q				<- NA
  hpc.mem				<- "6gb" 					# RAM	
  hpc.array			<- pty.c[, max(CASE_ID)]	# number of runs for job array
  
  #	define PBS header for job scheduler. this will depend on your job scheduler.
  pbshead		<- "#!/bin/sh"
  tmp			<- paste("#PBS -l walltime=", hpc.walltime, ":59:00,pcput=", hpc.walltime, ":45:00", sep = "")
  pbshead		<- paste(pbshead, tmp, sep = "\n")
  tmp			<- paste("#PBS -l select=", hpc.select, ":ncpus=", hpc.nproc,":mem=", hpc.mem, sep = "")
  pbshead 	<- paste(pbshead, tmp, sep = "\n")
  pbshead 	<- paste(pbshead, "#PBS -j oe", sep = "\n")	
  if(!is.na(hpc.array))
    pbshead	<- paste(pbshead, "\n#PBS -J 1-", hpc.array, sep='')	
  if(!is.na(hpc.q)) 
    pbshead <- paste(pbshead, paste("#PBS -q", hpc.q), sep = "\n")
  pbshead 	<- paste(pbshead, hpc.load, sep = "\n")	
  cat(pbshead)
  
  #	create PBS job array
  for (i in 1:pty.c[, max(JOB_ID)]) {
    tmp<-pty.c[JOB_ID==i,]
    cmd<-tmp[, list(CASE=paste0(CASE_ID,')\n',CMD,';;\n')), by='CASE_ID']
    cmd<-cmd[, paste0('case $PBS_ARRAY_INDEX in\n',paste0(CASE, collapse=''),'esac')]			
    cmd<-paste(pbshead,cmd,sep='\n')	
    outfile<-gsub(':','',paste("readali",paste0('job',i),paste(strsplit(date(),split=' ')[[1]],collapse='_',sep=''),'sh',sep='.'))
    outfile<-file.path(work.dir, outfile)
    cat(cmd, file=outfile)
    cmd<-paste("qsub", outfile)
    cat(cmd)
    cat(system(cmd, intern= TRUE))
  }
}

rkuvri.make.trees<- function() 
{
  # set up tree making scripts.
  # inputs: 
  #  in.dir: the directory that stores the alignments.
  # outputs:
  # generate and submit the scripts to make trees.
  
  require(data.table)
  require(Phyloscanner.R.utilities)
  #
  #	produce trees
  #
  # lightweight run
  if(0)	
  {
    hpc.select<- 1; hpc.nproc<- 1; hpc.walltime<- 4; hpc.mem<- "1850mb"; hpc.q<- NA
  }
  # midweight run 
  if(0)	
  {
    hpc.select<- 1; hpc.nproc<- 1; hpc.walltime<- 23; hpc.mem<- "1850mb"; hpc.q<- NA
  }
  # heavyweight run 
  if(1)	
  {
    # hpc.select<- 1; hpc.nproc<- 1; hpc.walltime<- 71; hpc.mem<- "5900mb"; hpc.q<- "pqeelab"
    hpc.select<- 1; hpc.nproc<- 1; hpc.walltime<- 71; hpc.mem<- "63850mb"; hpc.q<- NA
  }
  
  HOME <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/'	
  max.per.run <- 4900
  iqtree.pr <- 'iqtree'
  iqtree.args			<- ifelse(hpc.nproc==1, '-m GTR+F+R6 -ntmax 1 -seed 42 -o REF_CON_H', 
                          paste0('-m GTR+F+R6 -ntmax ',hpc.nproc,' -seed 42 -o REF_CON_H'))
  in.dir				<- file.path(HOME,'211220_phsc_output')
  out.dir				<- in.dir
  work.dir			<- file.path(HOME,"211220_phsc_work")
  
  # load alignments
  infiles	<- data.table(FI=list.files(in.dir, pattern='_v2.fasta$', full.names=TRUE, recursive=TRUE))
  infiles[, FO:= gsub('.fasta$','',FI)]
  infiles[, PTY_RUN:= as.integer(gsub('^ptyr([0-9]+)_.*','\\1',basename(FI)))]
  infiles[, W_FROM:= as.integer(gsub('.*InWindow_([0-9]+)_.*','\\1',basename(FI)))]		
  infiles[is.na(W_FROM),W_FROM:= as.integer(gsub('.*PositionsExcised_([0-9]+)_.*','\\1',basename(FI)))]
  infiles[,PositionsExcised:=grepl('PositionsExcised',FI)]
  setkey(infiles, PTY_RUN, W_FROM)
  tmp <- infiles[,list(NUM=length(PositionsExcised)),by=c('PTY_RUN', 'W_FROM')]
  infiles <- merge(infiles, tmp, by=c('PTY_RUN', 'W_FROM'))
  
  # delete the no excision file if the excision file exists
  tmp <- infiles[(PositionsExcised==F & NUM==2),]
  for (i in 1:nrow(tmp)) {
    cat('processing ',i,'\n')
    tmp_name<- tmp$FO[i]
    file.remove(paste0(tmp_name,'.fasta'))
    # file.remove(paste0(tmp_name,'.iqtree'))
    # file.remove(paste0(tmp_name,'.zip'))
  }
  
  infiles <- infiles[!(PositionsExcised==F & NUM==2),]
  # # rerun the unfinished jobs
  # infiles[,FO_NAME:=paste0(FO,'.treefile')]
  # infiles[,FO_EXIST:=file.exists(FO_NAME)]
  # infiles <- infiles[FO_EXIST==F]
  df<- infiles[, list(CMD=cmd.iqtree(FI, outfile=FO, pr=iqtree.pr, pr.args=iqtree.args)), by=c('PTY_RUN','W_FROM')]
  df[, ID:=ceiling(seq_len(nrow(df))/4)]
  df<- df[, list(CMD=paste(CMD, collapse='\n',sep='')), by='ID']
  
  #	create PBS job array
  if(nrow(df) > max.per.run){
    df[, CASE_ID:= rep(1:max.per.run,times=ceiling(nrow(df)/max.per.run))[1:nrow(df)]]
    df[, JOB_ID:= rep(1:ceiling(nrow(df)/max.per.run),each=max.per.run)[1:nrow(df)]]
    df[,ID:=NULL]
  }else{
    setnames(df, 'ID', 'CASE_ID')
    df[, JOB_ID:=1]
  }
  
  pbshead	<- cmd.hpcwrapper.cx1.ic.ac.uk(hpc.select=hpc.select, hpc.walltime=hpc.walltime, hpc.q=hpc.q, hpc.mem=hpc.mem,  hpc.nproc=hpc.nproc, hpc.load=NULL)
  
  for (i in 1:df[, max(JOB_ID)]) {
    tmp<-df[JOB_ID==i,]
    hpc.array <- nrow(tmp)
    cmd<-tmp[, list(CASE=paste0(CASE_ID,')\n',CMD,';;\n')), by='CASE_ID']
    cmd<-cmd[, paste0('case $PBS_ARRAY_INDEX in\n',paste0(CASE, collapse=''),'esac')]		
    tmp <- paste(pbshead, "\n#PBS -J 1-", hpc.array, sep='')	
    tmp <- paste(tmp,'module load anaconda3/personal \n source activate phylo' ,sep='\n')
    cmd<-paste(tmp,cmd,sep='\n')	
    
    #	submit jobs
    outfile	<- paste("srx",paste0('job',i),paste(strsplit(date(),split=' ')[[1]],collapse='_',sep=''),'sh',sep='.')
    outfile <- gsub(':','_',outfile)
    outfile<-file.path(work.dir, outfile)
    cat(cmd, file=outfile)
    cmd<-paste("qsub", outfile)
    cat(cmd)
    cat(system(cmd, intern= TRUE))
  }
}

make.phyloscanner<- function()
{
  # set up tree analysis scripts.
  # inputs: 
  #  treedir: the directory that stores the trees.
  # outputs:
  # generate and submit the scripts to analyse the trees. 
  
  require(tidyverse)
  require(data.table)
  require(phyloscannerR)
  
  # files
  HOME <<- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/'
  treedir <- file.path(HOME,'211220_phsc_output/')
  tmpdir	<- file.path(HOME,"211220_phsc_work/")
  outdir	<- file.path(HOME,'211220_phsc_phscrelationships_02_05_30_min_read_100_max_read_posthoccount_im_mrca_fixpd/')
  outdir	<- file.path(HOME,'211220_phsc_phscrelationships_02_05_30_min_read_null_max_read_posthoccount_im_mrca_fixpd/')
  outdir	<- file.path(HOME,'211220_phsc_phscrelationships_025_30_min_read_100_max_read_posthoccount_im_mrca_fixpd/')
  outdir	<- file.path(HOME,'211220_phsc_phscrelationships_025_30_min_read_null_max_read_posthoccount_im_mrca_fixpd/')
  dir.create(outdir)
  prog.phyloscanner_analyse_trees <- '/rds/general/user/xx4515/home/phyloscanner/phyloscanner_analyse_trees.R'
  
  #	set phyloscanner variables	
  control	<- list()
  control$allow.mt <- TRUE
  control$zero.length.adjustment = TRUE
  control$alignment.file.directory = NULL 
  control$alignment.file.regex = NULL
  control$blacklist.underrepresented = FALSE	
  control$count.reads.in.parsimony = TRUE
  control$distance.threshold <- '0.025'
  # control$distance.threshold <- '0.02 0.05'
  control$do.dual.blacklisting = FALSE					
  control$duplicate.file.directory = NULL
  control$duplicate.file.regex = NULL
  control$file.name.regex = "^(?:.*\\D)?([0-9]+)_to_([0-9]+).*$"
  control$guess.multifurcation.threshold = FALSE
  # control$max.reads.per.host <- 100
  control$max.reads.per.host <- NULL
  control$min.reads.per.host <- 30
  control$min.tips.per.host <- 1	
  control$multifurcation.threshold = 1e-5
  control$multinomial= TRUE
  control$norm.constants = NULL
  control$norm.ref.file.name = "~/normalisation_ByPosition.csv"
  control$norm.standardise.gag.pol = TRUE
  control$no.progress.bars = TRUE
  control$outgroup.name = "REF_CON_H"
  control$output.dir = outdir
  control$parsimony.blacklist.k = 15
  control$prune.blacklist = FALSE
  control$post.hoc.count.blacklisting <- TRUE
  control$ratio.blacklist.threshold = 0.01
  control$raw.blacklist.threshold = 3			
  control$recombination.file.directory = NULL
  control$recombination.file.regex = NULL
  control$relaxed.ancestry = TRUE
  control$sankoff.k = 15
  control$sankoff.unassigned.switch.threshold = 0
  control$seed = 42
  control$splits.rule = 's'
  control$tip.regex = '^(.*)-fq[0-9]+_read_([0-9]+)_count_([0-9]+)$'
  control$tree.file.regex = "^(.*)\\.treefile$" # from rscript
  control$treeFileExtension = '.treefile'
  control$use.ff = FALSE
  control$user.blacklist.directory = NULL 
  control$user.blacklist.file.regex = NULL
  control$verbosity = 1	
  
  #	make bash for many files	
  df <- tibble(F=list.files(treedir,pattern = 'ptyr*'))
  df <- df %>%
    mutate(TYPE:= gsub('ptyr([0-9]+)_(.*)','\\2', F),
           RUN:= as.integer(gsub('ptyr([0-9]+)_(.*)','\\1', F))) %>%
    mutate(TYPE:= gsub('^[^\\.]+\\.([a-z]+)$','\\1',TYPE)) %>%
    spread(TYPE, F) %>%
    set_names(~ str_to_upper(.))
  
  valid.input.args <- cmd.phyloscanner.analyse.trees.valid.args(prog.phyloscanner_analyse_trees)
  cmds <- vector('list',nrow(df))
  
  for(i in seq_len(nrow(df)))
  {
    #	set input args
    control$output.string <- paste0('ptyr',df$RUN[i])
    
    #	make script
    tree.input <- file.path(treedir, df$TREES[i])
    cmd <- cmd.phyloscanner.analyse.trees(prog.phyloscanner_analyse_trees, 
                                          tree.input, 
                                          control,
                                          valid.input.args=valid.input.args)
    cmds[[i]] <- cmd		
  }	
  cat(cmds[[1]])
  
  #
  # 	submit array job to HPC
  #
  #	make header
  hpc.load			<- "module load anaconda3/personal \n source activate phylor4"	# make third party requirements available	 
  hpc.select			<- 1						# number of nodes
  hpc.nproc			<- 1						# number of processors on node
  hpc.walltime		<- 23						# walltime
  if(1)		
  {
    hpc.q			<- NA						# PBS queue
    hpc.mem			<- "36gb" 					# RAM		
  }
  hpc.array			<- length(cmds)	# number of runs for job array	
  pbshead		<- "#!/bin/sh"
  tmp			<- paste("#PBS -l walltime=", hpc.walltime, ":59:00,pcput=", hpc.walltime, ":45:00", sep = "")
  pbshead		<- paste(pbshead, tmp, sep = "\n")
  tmp			<- paste("#PBS -l select=", hpc.select, ":ncpus=", hpc.nproc,":mem=", hpc.mem, sep = "")
  pbshead 	<- paste(pbshead, tmp, sep = "\n")
  pbshead 	<- paste(pbshead, "#PBS -j oe", sep = "\n")	
  if(!is.na(hpc.array))
    pbshead	<- paste(pbshead, "\n#PBS -J 1-", hpc.array, sep='')	
  if(!is.na(hpc.q)) 
    pbshead <- paste(pbshead, paste("#PBS -q", hpc.q), sep = "\n")
  pbshead 	<- paste(pbshead, hpc.load, sep = "\n")	
  cat(pbshead)
  
  #	make array job
  for(i in 1:length(cmds))
    cmds[[i]]<- paste0(i,')\n',cmds[[i]],';;\n')
  cmd		<- paste0('case $PBS_ARRAY_INDEX in\n',paste0(cmds, collapse=''),'esac')	
  cmd		<- paste(pbshead,cmd,sep='\n')	
  
  #	submit job
  outfile		<- gsub(':','',paste("phsc",paste(strsplit(date(),split=' ')[[1]],collapse='_',sep=''),'sh',sep='.'))
  outfile		<- file.path(tmpdir, outfile)
  cat(cmd, file=outfile)
  cmd 		<- paste("qsub", outfile)
  cat(cmd)
  cat(system(cmd, intern= TRUE))
}


phsc.transmission.networks<- function()
{
  # find transmission networks and chains from tree statistics.
  # inputs: tree analysis outputs.
  # outputs: transmission networks and transmission chains. 
  make_networks <- function(job_tag)
  {  
    library(data.table)
    library(tidyverse)
    
    # set up directories
    indir.base <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/211220_phsc_phscrelationships'
    indir	<- paste0(indir.base,job_tag)
    infiles	<- data.table(F=list.files(indir, pattern='*workspace.rda$', full.names=TRUE))
    infiles[, PTY_RUN:= as.integer(gsub('^ptyr([0-9]+)_.*','\\1',basename(F)))]
    setkey(infiles, PTY_RUN)

    #	collect all dwin and dc
    dca	<- infiles[, {
      cat(PTY_RUN,'\n')
      load(F)
      dc
    }, by='PTY_RUN']
    dwina <- infiles[, {
      cat(PTY_RUN,'\n')
      load(F)
      dwin
    }, by='PTY_RUN']
    save(dca,dwina,file = paste0('~/dcdwina',job_tag,'.rda'))

    # directories
    control <- list(linked.group='close.and.adjacent.cat',
                    linked.no='not.close.or.nonadjacent',
                    linked.yes='close.and.adjacent', 
                    dir.group = "close.and.adjacent.and.directed.cat",
                    conf.cut=0.6, 
                    neff.cut=3,
                    weight.complex.or.no.ancestry=0.5)
    indir.base <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/211220_phsc_phscrelationships'
    indir	<- paste0(indir.base,job_tag)
    
    library(tidyverse)
    library(glue)
    library(igraph)
    library(RBGL)
    library(data.table)
    library(phyloscannerR)
    load(paste0('~/dcdwina',job_tag,'.rda'))
    
    # control
    dca[,CNTRL1:=FALSE]
    dca[,CNTRL2:=FALSE]
    dca[grepl('CNTRL-',host.1),CNTRL1:=TRUE]
    dca[grepl('CNTRL-',host.2),CNTRL2:=TRUE]
    dca[grepl('CNTRL-',host.1),host.1:=gsub('CNTRL-','',host.1)]
    dca[grepl('CNTRL-',host.2),host.2:=gsub('CNTRL-','',host.2)]
    dwina[,CNTRL1:=FALSE]
    dwina[,CNTRL2:=FALSE]
    dwina[grepl('CNTRL-',host.1),CNTRL1:=TRUE]
    dwina[grepl('CNTRL-',host.2),CNTRL2:=TRUE]
    dwina[grepl('CNTRL-',host.1),host.1:=gsub('CNTRL-','',host.1)]
    dwina[grepl('CNTRL-',host.2),host.2:=gsub('CNTRL-','',host.2)]
    
    # sort dwin
    tmp			<- subset(dwina, host.1>host.2)
    setnames(tmp, c('host.1','host.2','paths12','paths21','nodes1','nodes2','CNTRL1','CNTRL2'),
             c('host.2','host.1','paths21','paths12','nodes2','nodes1','CNTRL2','CNTRL1'))
    set(tmp, NULL, 'close.and.contiguous.and.directed.cat',
        tmp[,gsub('xx','21',gsub('21','12',gsub('12','xx',close.and.contiguous.and.directed.cat)))])
    set(tmp, NULL, 'close.and.adjacent.and.directed.cat',
        tmp[,gsub('xx','21',gsub('21','12',gsub('12','xx',close.and.adjacent.and.directed.cat)))])
    set(tmp, NULL, 'close.and.contiguous.and.ancestry.cat',
        tmp[,gsub('xx','21',gsub('21','12',gsub('12','xx',close.and.contiguous.and.ancestry.cat)))])
    set(tmp, NULL, 'close.and.adjacent.and.ancestry.cat',
        tmp[,gsub('xx','21',gsub('21','12',gsub('12','xx',close.and.adjacent.and.ancestry.cat)))])
    dwina		<- rbind(subset(dwina, !(host.1>host.2)), tmp)
    
    # sort dca
    tmp			<- subset(dca, host.1>host.2)
    setnames(tmp, c('host.1','host.2','CNTRL1','CNTRL2'),
             c('host.2','host.1','CNTRL2','CNTRL1'))
    set(tmp, NULL, 'type',
        tmp[,gsub('xx','21',gsub('21','12',gsub('12','xx',type)))])
    dca		<- rbind(subset(dca, !(host.1>host.2)), tmp)  
    tmp <- unique(subset(dwina,select=c('PTY_RUN','host.1','host.2')))
    tmp <- tmp[,list(PTY_RUN=PTY_RUN[1]),by=c('host.1','host.2')]
    dwina <- merge(dwina,tmp, by=c('host.1','host.2'))
    dca <- merge(dca,tmp, by=c('host.1','host.2','PTY_RUN'))
    dwina$PTY_RUN.y=NULL
    dca$PTY_RUN.y=NULL
    setnames(dwina,'PTY_RUN.x','PTY_RUN',skip_absent=T)
    setnames(dca,'PTY_RUN.x','PTY_RUN',skip_absent=T)
    
    # find pairs
    tmp <- find.pairs.in.networks(dwina, dca, control=control, verbose=TRUE)
    dpl <- copy(tmp$network.pairs)
    dc <- copy(tmp$relationship.counts)
    dw <- copy(tmp$windows)
    save(dpl, dc, dw, file=file.path(indir,'Rakai_phscnetworks_allpairs.rda'))
    
    # find pairs
    # control <- list(linked.group='close.and.adjacent.cat',linked.no='not.close.or.nonadjacent',linked.yes='close.and.adjacent', conf.cut=0.5, neff.cut=3)
    # tmp <- find.pairs.in.networks(dwina, dca, control=control)
    # dpl <- copy(tmp$network.pairs)
    # dc <- copy(tmp$relationship.counts)
    # dw <- copy(tmp$windows)
    # save(dpl, dc, dw, file=file.path(indir,'Rakai_phscnetworksmh_allpairs.rda'))
    
    # find networks
    tmp <- find.networks(dc, control=control, verbose=TRUE)
    dnet <- copy(tmp$transmission.networks)
    dchain <- copy(tmp$most.likely.transmission.chains)
    save(dpl, dc, dw, dnet, dchain, file=file.path(indir,'Rakai_phscnetworks.rda'))

    # control<- list(linked.group='close.and.adjacent.cat',linked.no='not.close.or.nonadjacent',linked.yes='close.and.adjacent',
    #                dir.group="close.and.adjacent.and.ancestry.cat", neff.cut=3, weight.complex.or.no.ancestry=0.5)
    # tmp <- find.networks(dc, control=control, verbose=TRUE)
    # dnet <- copy(tmp$transmission.networks)
    # dchain <- copy(tmp$most.likely.transmission.chains)
    # save(dpl, dc, dw, dnet, dchain, file=file.path(indir,'Rakai_phscnetworksmh.rda'))
  }

  job_tag <- '_02_05_30_min_read_100_max_read_posthoccount_im_mrca_fixpd'
  job_tag <- '_02_05_30_min_read_null_max_read_posthoccount_im_mrca_fixpd'
  job_tag <- '_025_30_min_read_100_max_read_posthoccount_im_mrca_fixpd'
  job_tag <- '_025_30_min_read_null_max_read_posthoccount_im_mrca_fixpd'
  make_networks(job_tag)  
  
  library(data.table)
  indir <- '/rds/general/project/ratmann_deepseq_analyses/live/PANGEA2_RCCS1519_UVRI/210325_phsc_phscrelationships_02_05_30_min_read_100_max_read_posthoccount_im_mrca_fixpd'
  load(file.path(indir,'Rakai_phscnetworks.rda'))
  dchain <- as.data.table(dchain)
  dchain <- dchain[SCORE_LINKED>0.6]
  dchain[SCORE_DIR_12 <= 0.6 & SCORE_DIR_21 <= 0.6, EST_DIR:='unclear']
  dchain[SCORE_DIR_12 > 0.6, EST_DIR:='12']
  dchain[SCORE_DIR_21 > 0.6, EST_DIR:='21']
  table(dchain$EST_DIR)
  # 12      21 unclear 
  # 376     362     233 
  
}
